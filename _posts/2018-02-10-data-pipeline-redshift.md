---
layout: post
title: "Migrating Big Data to RedShift using AWS Data Pipeline"
date: 2018-02-10
author: Ramit Surana
category: big data data pipeline
tags: Big Data RedShift S3 Dynamodb MySQL
excerpt: "Building a Data Pipeline to transfer data to Redshift"
---

Hallo Freunde (German)(Hello Friends),As [Martin Fowler](http://martinfowler.com) correctly explains continous delivery:

> Continuous Delivery is a software development discipline where you build software in such a way that the software can be released to production at any time.The primary goal of the process is to be production ready anytime and anywhere.

In this simple and amazing piece of article we are going to discuss and explore some new amazing and rather interesting pieces technology.One i.e. Habitat,an Automation tool that Automates your process to build and publish Docker Images and Second i.e. Automate, which is a new chef CI/CD tool with a cool new dashboard & better features.

2 approaches:

1. External from one tool to another

2. Internal From One AWS Resource to another

## RedShift

## ELT vs ETL

## Internal Tools

### Data Pipeline

### CloudFormation

## External Tools

### Metallion

## Analytics

### Looker



